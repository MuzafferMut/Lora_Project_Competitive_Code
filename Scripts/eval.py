import torch
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
import random

# --- AYARLAR ---
# HocanÄ±n istediÄŸi zorunlu system prompt
SYSTEM_PROMPT = "You are an expert Python programmer. Please read the problem carefully before writing any Python code."

DATASETS = {
    "DEEP": "Naholav/CodeGen-Deep-5K",
    "DIVERSE": "Naholav/CodeGen-Diverse-5K"
}

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, required=True, choices=["DEEP", "DIVERSE"], help="Hangi datasetin test verisi?")
    parser.add_argument("--checkpoint", type=str, required=True, help="Test edilecek model klasÃ¶rÃ¼nÃ¼n yolu (Ã–rn: outputs/deep_final_model)")
    return parser.parse_args()

def main():
    args = parse_args()
    
    print(f"\nðŸ” MODEL DEÄžERLENDÄ°RÄ°LÄ°YOR...")
    print(f"Dataset: {args.dataset}")
    print(f"Model Yolu: {args.checkpoint}")
    
    # 1. EÄŸittiÄŸin Modeli YÃ¼kle (LoRA adaptÃ¶rleri ile birlikte)
    try:
        print("-> Model ve Tokenizer yÃ¼kleniyor...")
        tokenizer = AutoTokenizer.from_pretrained(args.checkpoint)
        model = AutoModelForCausalLM.from_pretrained(
            args.checkpoint,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
    except Exception as e:
        print(f"âŒ HATA: Model yÃ¼klenemedi. KlasÃ¶r yolunu doÄŸru yazdÄ±n mÄ±?\n{e}")
        return

    # 2. Test Verisini Ä°ndir
    print("-> Test verisi hazÄ±rlanÄ±yor...")
    full_dataset = load_dataset(DATASETS[args.dataset], split="train")
    
    # EÄŸitimde yaptÄ±ÄŸÄ±mÄ±z gibi %10'luk test kÄ±smÄ±nÄ± ayÄ±rÄ±yoruz
    # (Seed 42 veriyoruz ki eÄŸitimdeki test setiyle aynÄ± olsun)
    split_dataset = full_dataset.train_test_split(test_size=0.1, seed=42)
    test_dataset = split_dataset["test"]
    
    print(f"-> Toplam {len(test_dataset)} test sorusu var. Rastgele 3 tanesi seÃ§iliyor...")
    
    # Rastgele 3 Ã¶rnek seÃ§
    indices = random.sample(range(len(test_dataset)), 3)
    
    for i in indices:
        example = test_dataset[i]
        input_text = example["input"]
        ground_truth = example["solution"]
        
        # Prompt HazÄ±rla (Chat FormatÄ±nda)
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": input_text}
        ]
        
        # Modelin anlayacaÄŸÄ± formata Ã§evir
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # Model Ãœretsin
        print(f"\n{'='*40}")
        print(f"SORU (Ã–zet): {input_text[:150]}...")
        print(f"{'-'*40}")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=512,
                temperature=0.2, # Daha tutarlÄ± kod yazmasÄ± iÃ§in dÃ¼ÅŸÃ¼k sÄ±caklÄ±k
                top_p=0.9,
                do_sample=True
            )
        
        # CevabÄ± Temizle (Sadece yeni Ã¼retilen kÄ±smÄ± al)
        generated_code = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        
        print(f"ðŸ¤– MODELÄ°N CEVABI:\n{generated_code}")
        print(f"\nâœ… GERÃ‡EK CEVAP (Referans):\n{ground_truth}")
        print(f"{'='*40}\n")

    print("Test TamamlandÄ±.")

if __name__ == "__main__":
    main()

#python scripts/eval.py --dataset DEEP --checkpoint outputs/deep_final_model

#python scripts/eval.py --dataset DIVERSE --checkpoint outputs/diverse_final_model 