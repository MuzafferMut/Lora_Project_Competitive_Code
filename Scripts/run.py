import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time

# KullanÄ±lacak Model
model_id = "Qwen/Qwen2.5-Coder-1.5B-Instruct"

print(f"--- MODEL BAÅLATILIYOR ---\nModel: {model_id}")

# 1. Modeli YÃ¼kleme (Disk -> GPU)
try:
    start_time = time.time()
    
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16, # RTX 5060 performansÄ± iÃ§in
        device_map="auto"           # Otomatik olarak GPU'ya atar
    )
    
    load_time = time.time() - start_time
    print(f"âœ… Model GPU'ya yÃ¼klendi ({load_time:.2f} saniye sÃ¼rdÃ¼).")
    
except Exception as e:
    print(f"âŒ HATA: Model yÃ¼klenemedi. Ã–nce download.py Ã§alÄ±ÅŸtÄ±rdÄ±n mÄ±?\nHata: {e}")
    exit()

# 2. Test Sorusu (HocanÄ±n istediÄŸi inference testi)
prompt = "Write a Python function to calculate the Fibonacci sequence up to n terms."

messages = [
    {"role": "system", "content": "You are a helpful coding assistant."},
    {"role": "user", "content": prompt}
]

# 3. HazÄ±rlÄ±k ve Ãœretim
input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

print("\nğŸ¤– Model DÃ¼ÅŸÃ¼nÃ¼yor (Inference)...")
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512
)

# 4. Ã‡Ä±ktÄ±yÄ± Temizleme
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

print("-" * 40)
print(f"SORU: {prompt}")
print("-" * 40)
print(response)
print("-" * 40)